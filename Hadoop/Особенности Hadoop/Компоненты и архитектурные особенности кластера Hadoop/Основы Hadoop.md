# Основы Hadoop

## Что такое Hadoop

***Apache Hadoop*** это open source платформа, набор программ и утилит, предназначенных для хранения и обработки больших объемов данных. Среда хранения данных является распределенной. Целью Hadoop является обеспечения высокой эффектиности работы и управления с большими данными, обработка которых на одной машине невозможна или труднодостижима.

## Из чего состоит

### Основные компоненты
Hadoop состоит из нескольких компонентов:
- ***HDFS - Hadoop Distributed File System*** - распределенная файловая система *Hadoop*, которая хранит данные на нескольких серверах. Благодаря репликации данных достигается высокая отказоустойчивость системы хранения, что позволяет избежать потерь при отказе одного из блоков хранения.
- ***YARN - Yet Another Resource Negotiator*** - система управления ресурсами и планирования задач. *YARN* позволяет эффективно распределять задачи между узлами кластера, управляя вычислительными ресурсами.
- ***MapReduce*** - парадигма обработки данных, состоящая из двух этапов: первый этап подразумевает разбиения задачи на мелкие подзадачи(этап *Map*), выполняемые параллельно в разных узлах, после чего во втором этапе результат будет объединен для получения финального ответа(этап *Reduce*).
- ***Hadoop Common*** - набор утилит, библиотек и инструментов, необходимых для создания инфраструктуры. В "*Common*" входит множество инструментов, например, "*Common Configuration*", предоставляющий возможность настройки Hadoop-приложений через файлы XML. Или "*Common I/O*", который контролирует взаимодействие между различными файловыми системами, например между "*HDFS*" и "*Amazon S3*".

### Дополнительные компоненты экосистемы Hadoop

В экосистему *Hadoop* входит множество компонентов, облегчающих работу и позволяющих проводить множество различных операций над данными или с источниками данных.

- ***Hive*** - СУБД, инструмент для SQL-подобных запросов к большим данным, преобразующий запросы в серию *MapReduce* задач. Позволяет читать, записывать и управлять массивами данных, размещенных в распределенном хранилище.

- ***Pig*** - платформа анализа больших данных, представляющая их в виде одного единого потока данных.

- ***HBase*** - NoSQL БД, работающая поверх Hadoop кластера, обеспечивающая работу операций чтения и записи над большими массивами данных.

- ***ZooKeeper*** - сервис для координации распределенных систем и управления ими.

- ***Oozie*** - система планирования рабочих процессов для управления заданиями *Hadoop*.

- ***Apache Spark*** - фреймворк, реализующий распределенную обработку данных, который постепенно приходит на смену *MapReduce* вследствие возросшей скорости обработки данных, простоты использования, возможностям машинного обучения и потоковой передачи данных в режиме реального времени.

## Как работает *Hadoop*
Как говорилось ранее, *Hadoop* следует модели работы с данными *MapReduce*. В целом суть работы *Hadoop* заключается в параллельной обработке данных на множестве разделенных узлов. Каждый из узлов выполняет план работы *MapReduce* над своим кластером данных, после чего результаты собираются воедино и формируется итоговый результат. Таким образом задача обработки больших данных разделяется на ряд мелких операций и подзадач выполняемых параллельно, вследствие чего возрастает эффективность работы.
Общий процесс работы включает следующие шаги:

1. Разделение данных. При поступлении в систему, данные разделяются на маленькие фрагменты, после чего записываются в разные узлы кластера.
2. Обработка данных. Над каждым фрагментом в кластерах производится операция *Map*, в результате которой над данными производятся некоторые операции: сортировка, фильтрация, анализ и т.д. Также в этом этапе генерируются пары "ключ - значение", которые сохраняются в промежуточном хранилище.
3. Перераспределение данных. Данные перераспределяются и сортируются таким образом, чтобы все данные с одинаковым ключом оказались в составе одного узла. Затем происходит сортировка данных по ключам.
4. Сбор результатов и их объединение. Согласно этапу *Reduce*, результаты обработки фрагментов данных собираются воедино из рабочих узлов(происходит свертка) в главный узел для получения полноценного результата.

## Какие преимущества
Работа с большими данными в *Hadoop* предоставляет целый ряд преимуществ:
- *эффективность и экономичность* - обработка больших данных не требует специфического оборудования, а распределение нагрузки позволяет быстрее обрабатывать массив значений.
- *масштабируемость* - *Hadoop* не ограничивает возможности расширения инфраструктуры для обработки данных, ровно как и не ограничивает массив обрабатываемых данных в размерах.
- *отказоустойчивость* - распределение данных в случае отказа на множестве рабочих узлов(репликация данных), вкупе с автоматическим восстановлением после сбоев позволяет построить отказоустойчивый кластер хранения и обработки данных.
- *универсальность* - *Hadoop* позволяет работать как со структурированными, так и с полуструктурированными или неструктурированными данными, также поддерживает множество различных форматов, что позволяет работать с данными разного рода и получать их из разных источников.