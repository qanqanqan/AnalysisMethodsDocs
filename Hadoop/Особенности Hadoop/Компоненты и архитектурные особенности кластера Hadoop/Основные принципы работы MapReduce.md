## Основные принципы работы MapReduce

**MapReduce** — это модель распределённых вычислений, предназначенная для обработки и анализа больших объёмов данных в распределённых системах. Основная идея заключается в параллельной обработке данных на множестве узлов кластера, что позволяет эффективно работать с данными, достигающими нескольких петабайт.

### Структура работы MapReduce

Модель MapReduce состоит из нескольких ключевых этапов, которые обеспечивают эффективную обработку больших объёмов данных в распределённых системах. Основные этапы включают:

### Основные этапы

- **Map (отображение)**: На этом этапе входные данные разбиваются на части. Главный узел (master node) получает данные, делит их и передаёт рабочим узлам (worker nodes) для предварительной обработки. Каждый узел применяет функцию `map`, генерируя пары "ключ-значение". 

- **Reduce (свёртка)**: После обработки данных на этапе Map, результаты собираются и обрабатываются. Рабочие узлы получают сгруппированные данные по ключам и применяют функцию `reduce`, чтобы свести все значения для одного ключа к единому результату.

### Дополнительные этапы

- **Shuffle**: Этот этап происходит между Map и Reduce. На нём данные перераспределяются так, чтобы все значения с одинаковыми ключами обрабатывались одним рабочим узлом. Это обеспечивает корректность обработки и оптимизацию свёртки.

### Принцип работы

1. **Получение данных**: Главный узел получает массив данных, который необходимо обработать.
  
2. **Разделение данных**: Данные разбиваются на части, которые распределяются между рабочими узлами.

3. **Параллельная обработка**: Каждый рабочий узел выполняет функцию `map` над своей частью данных.

4. **Перераспределение данных**: После этапа Map происходит Shuffle, где данные перераспределяются по ключам.

5. **Сбор результатов**: На этапе Reduce рабочие узлы обрабатывают сгруппированные данные и отправляют результаты обратно главному узлу.

6. **Финальный результат**: Главный узел собирает все результаты и формирует окончательный ответ на задачу.
