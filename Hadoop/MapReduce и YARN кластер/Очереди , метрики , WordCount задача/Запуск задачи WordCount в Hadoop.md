Что такое задача WordCount в Hadoop и как она работает?
**WordCount** — это одна из самых простых и часто используемых демонстрационных задач для объяснения принципов работы Hadoop. Задача WordCount предназначена для подсчета количества вхождений каждого слова в наборе текстовых файлов, используя парадигму **MapReduce** — основную модель обработки данных в Hadoop.

### Как работает задача WordCount в Hadoop:

Задача WordCount включает два основных этапа — **Map** и **Reduce**, которые вместе реализуют процесс распределенной обработки данных.

#### 1. **Этап Map (отображение)**

На этапе Map данные разбиваются на небольшие куски и обрабатываются параллельно на различных узлах (ноды) в кластере. Каждый узел выполняет одну и ту же операцию над своим фрагментом данных.

- **Входные данные:** Файл(ы) с текстом, которые нужно обработать. Например, текстовый файл, содержащий строки, такие как:
  
  ```
  Hadoop is great
  Hadoop is scalable
  ```

- **Процесс Map:** Каждый узел-Mapper берет строку текста, разбивает ее на отдельные слова и генерирует пары ключ-значение, где ключ — это слово, а значение — 1 (так как каждое слово встретилось один раз).
  
  Пример вывода Map:
  ```
  (Hadoop, 1)
  (is, 1)
  (great, 1)
  (Hadoop, 1)
  (is, 1)
  (scalable, 1)
  ```

#### 2. **Этап Shuffle and Sort (перетасовка и сортировка)**

После завершения этапа Map, промежуточные данные (пары ключ-значение) передаются к этапу Reduce. Перед этим данные сортируются по ключам и группируются таким образом, чтобы все записи с одинаковым ключом (словом) находились вместе.

Пример после перетасовки и сортировки:
```
(Hadoop, [1, 1])
(is, [1, 1])
(great, [1])
(scalable, [1])
```

#### 3. **Этап Reduce (сведение)**

На этапе Reduce каждый узел-Reducer берет сгруппированные данные и обрабатывает их для вычисления окончательного результата.

- **Процесс Reduce:** Для каждого ключа (слова) Reducer получает список значений (чисел 1) и суммирует их, чтобы получить общее количество вхождений этого слова.

Пример вывода Reduce:
```
(Hadoop, 2)
(is, 2)
(great, 1)
(scalable, 1)
```

Таким образом, получаем итоговый результат подсчета количества вхождений каждого слова.

#### 4. **Финальный результат**

Выходные данные содержат список уникальных слов и количество раз, сколько каждое слово встретилось в наборе входных файлов:
```
Hadoop    2
is        2
great     1
scalable  1
```

### Основные шаги задачи WordCount:

1. **Разбиение данных:** Входные данные (файл или набор файлов) разбиваются на части для параллельной обработки.
2. **Map (Отображение):** Каждая строка обрабатывается и разбивается на ключи (слова) с присвоением значения 1 каждому ключу.
3. **Shuffle and Sort (Перетасовка и сортировка):** Сгруппированные и отсортированные по ключу данные передаются Reducer'у.
4. **Reduce (Сведение):** Каждый уникальный ключ (слово) обрабатывается, и его значения суммируются.
5. **Результат:** Выходные данные включают каждое уникальное слово и количество его появлений.

### Почему задача WordCount важна?
WordCount — это классический пример, показывающий, как модель MapReduce может использоваться для решения задач, которые требуют распределенной обработки больших объемов данных. Этот алгоритм прост в реализации, но его концепция применима к более сложным задачам обработки данных, таким как сортировка, агрегация и фильтрация данных в больших кластерах Hadoop.