
Hadoop — это мощная платформа для хранения и обработки больших объемов данных, состоящая из нескольких ключевых компонентов. Основные компоненты Hadoop, которые обеспечивают хранение и управление данными, включают:
1. **Hadoop Distributed File System (HDFS)**
2. **Yet Another Resource Negotiator (YARN)**
3. **MapReduce**
4. **Hadoop Common** - набор общих утилит и библиотек, необходимых для работы всех других компонентов Hadoop.

HDFS обеспечивает надежное хранение, YARN управляет ресурсами, а MapReduce позволяет эффективно обрабатывать данные. 

Дополнительные инструменты:
- **Hive**: инструмент для анализа данных с использованием SQL-подобного языка запросов.
- **HBase**: распределенная NoSQL база данных, работающая поверх HDFS.
- **Pig**: высокоуровневый язык скриптов для обработки больших данных.
- **Oozie**: система управления рабочими процессами для автоматизации задач обработки данных.

---


1. **Hadoop Distributed File System (HDFS)**:
    
    - **Описание**: HDFS является основным компонентом для хранения данных в Hadoop. Он распределяет данные по множеству узлов, обеспечивая высокую доступность и отказоустойчивость.
    - **Архитектура**: HDFS использует архитектуру "мастер-слейв", где **NameNode** (мастер) управляет метаданными и структурой файловой системы, а **DataNodes** (слейвы) хранят фактические данные в виде блоков.
    - **Функции**: HDFS разбивает большие файлы на более мелкие блоки (обычно по 128 МБ или 256 МБ) и реплицирует их на нескольких узлах для обеспечения надежности.
    
2. **Yet Another Resource Negotiator (YARN)**:
    
    - **Описание**: YARN отвечает за управление ресурсами в кластере и распределение задач. Он позволяет различным обработчикам данных (например, MapReduce, Spark) работать на одной платформе.
    - **Архитектура**: YARN разделяет функции управления ресурсами и планирования задач на два основных компонента: **ResourceManager** (управляет ресурсами на уровне кластера) и **NodeManager** (управляет ресурсами на уровне узла).
    - **Функции**: YARN обеспечивает эффективное использование ресурсов, позволяя одновременно обрабатывать различные типы нагрузок (пакетная обработка, интерактивные запросы и т.д.).
    
3. **MapReduce**:
    
    - **Описание**: MapReduce — это программная модель для обработки больших объемов данных параллельно. Она разбивает задачу на две основные фазы: Map (преобразование данных) и Reduce (агрегация результатов).
    - **Функции**: MapReduce позволяет обрабатывать данные в распределенной среде, что делает его важным компонентом для анализа больших наборов данных.
    
4. **Hadoop Common**:
    
    - **Описание**: Это набор общих утилит и библиотек, необходимых для работы всех других компонентов Hadoop.
    - **Функции**: Hadoop Common включает в себя библиотеки для работы с файловой системой, сериализации данных и управления конфигурацией.
    
---

## Дополнительные компоненты

Кроме основных компонентов, в экосистему Hadoop входят и другие инструменты:

- **Hive**: инструмент для анализа данных с использованием SQL-подобного языка запросов.
- **HBase**: распределенная NoSQL база данных, работающая поверх HDFS.
- **Pig**: высокоуровневый язык скриптов для обработки больших данных.
- **Oozie**: система управления рабочими процессами для автоматизации задач обработки данных.
---

