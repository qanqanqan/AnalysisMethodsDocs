В Apache Spark все операции делятся на два основных типа: **трансформации** и **действия**. Понимание этих понятий является ключевым для работы с данными в Spark, а также для оптимизации производительности за счет правильного использования ленивого вычисления и создания плана запросов.

### Трансформации (Transformations):
Трансформации — это операции, которые создают новое представление данных, но фактически не выполняются до тех пор, пока не будет вызвано действие. Это часть концепции ленивого вычисления в Spark: когда вы применяете трансформации к RDD или DataFrame, Spark строит план выполнения (DAG — Directed Acyclic Graph), который будет оптимизирован и выполнен позже.

Примеры трансформаций:
- **map()**: Применяет функцию к каждому элементу RDD или DataFrame.
- **filter()**: Оставляет только те элементы, которые удовлетворяют определенному условию.
- **flatMap()**: Преобразует каждый элемент в коллекцию и затем "разворачивает" результат в одну последовательность.
- **groupByKey()**: Группирует данные по ключу.
- **join()**: Объединяет два RDD или DataFrame на основе ключа.

Важно отметить, что трансформации ленивые: они просто добавляют шаги к плану выполнения, не запуская реальных вычислений.

### Действия (Actions):
Действия — это операции, которые заставляют Spark выполнить все накопленные трансформации и вернуть результат или сохранить его. Когда действие вызывается, Spark запускает выполнение всех операций, используя оптимизированный план выполнения, построенный на основе трансформаций.

Примеры действий:
- **collect()**: Собирает все элементы RDD или DataFrame в драйвере (осторожно с большими объемами данных).
- **count()**: Возвращает количество элементов.
- **first()**: Возвращает первый элемент.
- **take(n)**: Возвращает первые n элементов.
- **saveAsTextFile()**: Сохраняет данные в текстовый файл.
- **foreach()**: Применяет функцию к каждому элементу без возврата результата.

### Взаимосвязь с оптимизацией запросов:
Поскольку трансформации не выполняются сразу, а накапливаются в виде плана, это позволяет Spark оптимизировать выполнение запроса перед фактическим запуском. Spark использует оптимизатор **Catalyst**, который преобразует логический план (созданный на основе трансформаций) в физический план, пытаясь минимизировать количество шагов и улучшить производительность. Например, Catalyst может перетасовать операции так, чтобы фильтрация данных выполнялась до объединений (join), что может сократить объем данных, участвующих в шифлинге.

### Заключение:
Трансформации в Spark — это ленивые операции, которые создают план выполнения запроса, а действия запускают этот план на выполнение. Для эффективной работы важно правильно проектировать последовательность трансформаций, чтобы минимизировать объем данных, уменьшать шифлинг и кэшировать данные при необходимости для ускорения повторных вычислений.