Apache Spark — это распределенная платформа для обработки данных, которая поддерживает параллельное вычисление и масштабируемость для больших объемов данных. Она разработана для работы на кластерах и предоставляет API на нескольких языках, включая Python (через PySpark), Java, Scala и R.

Spark работает на основе концепции **RDD** (Resilient Distributed Dataset), которая представляет собой распределенные коллекции объектов, которые могут быть обработаны параллельно. Важным аспектом работы со Spark является **ленивое вычисление**: операции не выполняются немедленно, а накапливаются в виде плана вычислений (плана запроса), который позже может быть оптимизирован и выполнен более эффективно. Это позволяет Spark избегать ненужных вычислений и лучше управлять ресурсами.

### Оптимизация запросов в Spark
1. **Кэширование**: Часто используемые данные можно закэшировать в памяти, чтобы ускорить повторные вычисления.
2. **Проектирование планов выполнения**: Spark автоматически оптимизирует запросы через свой оптимизатор **Catalyst**, который преобразует запрос в план, выбирая наиболее эффективную стратегию.
3. **Слияние и фильтрация данных**: Избегайте ненужных шифровок (shuffling) данных, заранее фильтруйте и объединяйте данные на ранних этапах обработки.
4. **Использование DataFrame API**: Этот API предоставляет более высокоуровневую абстракцию, которая позволяет Spark оптимизировать выполнение запросов более эффективно, чем с использованием RDD.

Таким образом, основы Spark заключаются в понимании его распределенной архитектуры, использования ленивого вычисления и способов оптимизации запросов для работы с большими данными.