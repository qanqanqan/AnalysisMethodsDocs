
## Создание RDD

Существует два основных способа создания RDD:

1. **Параллелизация существующей коллекции**: Вы можете создать RDD из локальной коллекции, используя метод `parallelize` объекта `SparkContext`. Например:
```python
from pyspark import SparkContext 
sc = SparkContext("local", "Example") 
data = [1, 2, 3, 4, 5] rdd = sc.parallelize(data)
```
    
2. **Загрузка из внешних источников**: RDD также можно создать, загружая данные из файловой системы (например, HDFS) с помощью метода `textFile`. Например:
```python
rdd = sc.textFile("hdfs://path/to/file.txt")
```
---

## Операции над RDD

RDD поддерживает два типа операций: **трансформации** и **действия**.

### Трансформации

Трансформации создают новый RDD из существующего. Они ленивые, то есть вычисления выполняются только при необходимости. Примеры трансформаций:

- `map(func)`: применяет функцию к каждому элементу RDD.
- `filter(func)`: возвращает новый RDD, содержащий элементы, которые удовлетворяют условию функции.
- `flatMap(func)`: применяет функцию и возвращает плоский список результатов.

### Действия

Действия выполняют вычисления и возвращают результат. Примеры действий:

- `collect()`: возвращает все элементы RDD в виде списка.
- `count()`: возвращает количество элементов в RDD.
- `reduce(func)`: объединяет элементы RDD с помощью функции.
---
## Устойчивость и управление памятью

Одним из ключевых преимуществ RDD является их отказоустойчивость. Если узел выходит из строя, Spark может восстановить утраченные данные на основе логов операций. Кроме того, RDD может быть сохранен в памяти для ускорения последующих операций с помощью метода `persist()` или `cache()`. 

Это позволяет избежать повторного вычисления при многократном использовании одного и того же RDD.

---
## Пример работы с RDD

Вот пример простого приложения на Python с использованием PySpark:

```python
from pyspark import SparkContext 
sc = SparkContext("local", "Word Count Example") 
# Создание RDD из текстового файла 
rdd = sc.textFile("hdfs://path/to/textfile.txt") 
# Применение трансформаций и действий 
word_counts = rdd.flatMap(lambda line: line.split(" ")) \                  
	.map(lambda word: (word, 1)) \                 
	.reduceByKey(lambda a, b: a + b) 
	
# Сбор результатов 
results = word_counts.collect() 
for word, count in results:     
	print(f"{word}: {count}")
```
Этот код загружает текстовый файл, разбивает его на слова и подсчитывает количество вхождений каждого слова.
