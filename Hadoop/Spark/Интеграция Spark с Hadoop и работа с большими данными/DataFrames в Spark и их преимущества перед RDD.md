DataFrames в Spark представляют собой распределенные коллекции данных, организованные в виде таблиц с именованными столбцами, что делает их более похожими на традиционные базы данных, по сравнению с RDD. Основные преимущества DataFrames перед RDD:

 1. Оптимизация производительности: DataFrame использует Catalyst Optimizer, который автоматически оптимизирует план выполнения запросов, что улучшает производительность.
 2. Использование API на высоком уровне: DataFrame предоставляет API на более высоком уровне абстракции, что упрощает работу с большими данными. Пользователи могут использовать знакомые SQL-подобные операции, такие как select(), filter(), и groupBy().
 3. Поддержка различных источников данных: DataFrames могут напрямую работать с различными форматами данных, такими как JSON, CSV, Parquet, и ORC.
 4. Интероперабельность с SQL: DataFrames можно использовать с SQL-запросами, что облегчает интеграцию с традиционными системами управления базами данных (СУБД).
 5. Улучшенное управление памятью: DataFrame эффективнее использует память за счет схемы и компактного хранения данных, в то время как RDD хранит данные менее эффективно.

Пример создания DataFrame в PySpark:
```
from pyspark.sql import SparkSession

# Инициализация сессии Spark
spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()

# Чтение данных из CSV файла и создание DataFrame
df = spark.read.csv("hdfs://namenode:9000/data/file.csv", header=True, inferSchema=True)

# Пример простого SQL-запроса через DataFrame API
df.select("column_name").filter(df["age"] > 30).show()
```
В этом примере DataFrame предоставляет простой и эффективный способ обработки данных с возможностью интеграции с Hadoop через HDFS.