В Spark используется принцип ленивых вычислений (lazy evaluation), что означает, что операции над данными (например, трансформации) не выполняются сразу, как они определены. Вместо этого Spark строит DAG (направленный ациклический граф) операций, который исполняется только в момент вызова действия (action), например, count(), collect(), saveAsTextFile(). Такой подход помогает оптимизировать выполнение задач и уменьшить количество операций с данными, поскольку Spark может объединять несколько трансформаций в одно выполнение.

Отказоустойчивость в Spark обеспечивается через механизм восстановления данных (fault tolerance), который основан на концепции RDD (Resilient Distributed Dataset). RDD отслеживает операции трансформации через lineage (родословную), что позволяет восстановить данные в случае сбоя. Если узел кластера выйдет из строя или произойдет ошибка при выполнении задачи, Spark может пересчитать потерянные данные, используя lineage, без необходимости перезапускать всю задачу.

Пример кода на PySpark:
```python
from pyspark import SparkContext

# Инициализация SparkContext
sc = SparkContext("local", "Lazy Evaluation Example")

# Пример ленивых вычислений
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Трансформация (ленивая операция)
rdd_transformed = rdd.map(lambda x: x * 2)

# Действие (вызывает выполнение всех предыдущих трансформаций)
result = rdd_transformed.collect()

print(result)  # Вывод: [2, 4, 6, 8, 10]
```
# Пример отказоустойчивости
# Допустим, произошел сбой на одном из узлов
# Spark автоматически восстановит потерянные данные, используя lineage

Таким образом, ленивые вычисления помогают оптимизировать выполнение задач, а отказоустойчивость гарантирует, что данные не будут потеряны при сбоях.