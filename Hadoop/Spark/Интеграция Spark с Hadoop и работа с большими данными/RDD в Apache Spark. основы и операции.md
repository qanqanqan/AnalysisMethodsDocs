RDD (Resilient Distributed Dataset) — это основная абстракция в Apache Spark, которая представляет собой распределённую коллекцию объектов, способную выполняться параллельно на нескольких узлах кластера. Основные свойства RDD:

 1. Устойчивость (Resilient): Если данные теряются из-за сбоя узла, RDD может быть восстановлен из исходных данных благодаря сохранённым операциям.
 2. Распределённость (Distributed): Данные хранятся и обрабатываются на нескольких узлах.
 3. Иммутабельность (Immutable): После создания RDD не может быть изменён. Для изменения данных необходимо создавать новый RDD.

Основные операции с RDD:

 1. Трансформации (Transformations): Это “ленивые” операции, которые создают новый RDD на основе существующего. Трансформации не приводят к немедленному выполнению, а лишь описывают вычислительные задачи.
 • map(): применяет функцию к каждому элементу RDD.
 • filter(): выбирает элементы, удовлетворяющие условию.
 • flatMap(): похож на map(), но может вернуть несколько выходных элементов для одного входного.
 2. Действия (Actions): Это операции, которые запускают вычисления и возвращают результат.
 • collect(): собирает все элементы RDD на драйвере.
 • count(): возвращает количество элементов в RDD.
 • reduce(): сводит все элементы RDD с помощью переданной функции.

Пример работы с RDD в PySpark:
```python
from pyspark import SparkContext

sc = SparkContext("local", "RDD Example")

# Создание RDD
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Применение трансформаций
rdd_transformed = rdd.map(lambda x: x * 2).filter(lambda x: x > 5)

# Применение действия
result = rdd_transformed.collect()

print(result)  # Вывод: [6, 8, 10]
```
Этот пример показывает создание RDD, выполнение трансформаций (умножение на 2 и фильтрация) и сбор результата на драйвере с помощью действия collect().