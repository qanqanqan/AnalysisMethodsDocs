Spark и Hadoop — это две мощные платформы для работы с большими данными, но у них есть несколько ключевых отличий:

 1. Обработка данных:
 • Hadoop: использует MapReduce — дисковую модель, при которой данные сохраняются на диск после каждой операции. Это делает обработку данных более медленной из-за высокой затратности операций ввода-вывода (I/O).
 • Spark: работает в памяти, что позволяет выполнять большинство операций быстрее, так как данные не записываются на диск между шагами (если не требуется). Это делает Spark значительно быстрее для обработки данных в памяти, особенно для итеративных задач.
 2. Скорость:
 • Hadoop: относительно медленнее из-за использования MapReduce, поскольку каждая итерация требует чтения и записи данных на диск.
 • Spark: значительно быстрее, особенно при итеративных вычислениях, так как работает преимущественно в памяти.
 3. Обработка данных в реальном времени:
 • Hadoop: в основном используется для пакетной обработки данных. Для обработки данных в реальном времени требуются дополнительные инструменты, такие как Apache Storm.
 • Spark: поддерживает как пакетную, так и потоковую обработку данных через Spark Streaming, что делает его более универсальным для различных сценариев.
 4. Управление отказами:
 • Hadoop: использует HDFS (Hadoop Distributed File System), который сохраняет данные на диск, что помогает восстановить их после сбоев.
 • Spark: имеет механизм восстановления через DAG (Directed Acyclic Graph) и может восстановить потерянные данные, но основное внимание уделяется вычислениям в памяти. Тем не менее, для критических данных рекомендуется использовать HDFS или другой устойчивый хранилище.
 5. Экосистема:
 • Hadoop: большая экосистема, включающая компоненты для хранения данных (HDFS), управления кластерами (YARN), потоковой обработки (Storm), анализа данных (Hive, Pig), и другие.
 • Spark: предоставляет единое API для различных задач, таких как SQL-запросы, машинное обучение, потоковая обработка и графовые вычисления.

Пример интеграции Spark с HDFS:
```python
from pyspark.sql import SparkSession

# Создание SparkSession с подключением к HDFS
spark = SparkSession.builder \
    .appName("SparkHDFSExample") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
    .getOrCreate()

# Чтение данных из HDFS
df = spark.read.csv("hdfs://namenode:9000/user/data/large_dataset.csv")

# Выполнение операций над данными
df_filtered = df.filter(df['column'] > 100)

# Сохранение результата обратно в HDFS
df_filtered.write.csv("hdfs://namenode:9000/user/data/output_filtered.csv")
```
Spark можно легко интегрировать с экосистемой Hadoop для обработки больших данных с использованием HDFS для хранения и YARN для управления ресурсами.