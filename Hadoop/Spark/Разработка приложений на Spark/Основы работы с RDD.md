RDD (Resilient Distributed Dataset) — это основная абстракция данных в Apache Spark. Это распределённая коллекция объектов, которая может быть обработана параллельно на кластере. Основные характеристики RDD:

1. **Устойчивость (Resilient)**: RDD обеспечивает отказоустойчивость через хранение информации о том, как его заново создать в случае потери данных. Это реализуется через журнализация (lineage), которая отслеживает операции, применённые к RDD.
   
2. **Распределённость (Distributed)**: RDD хранится на нескольких узлах кластера, что позволяет обрабатывать большие объёмы данных параллельно, повышая скорость вычислений.

3. **Набор данных (Dataset)**: RDD представляет собой коллекцию данных, которая может быть преобразована или обработана с помощью различных операций.

Основы работы с RDD:

1. **Создание RDD**: RDD можно создать несколькими способами:
   - Чтение данных из внешнего источника (например, HDFS, S3, локальные файлы).
   - Преобразование существующего RDD (например, фильтрация, маппинг данных).
   - Параллелизация коллекции (например, создание RDD из локального массива данных).
   
   Пример создания RDD из коллекции:
   ```python
   rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
   ```

2. **Трансформации**: Это ленивые операции, которые создают новое RDD, но не запускают вычисления. Например:
   - `map`: применяет функцию к каждому элементу RDD и возвращает новый RDD.
     ```python
     rdd2 = rdd.map(lambda x: x * 2)
     ```
   - `filter`: отбирает элементы, удовлетворяющие условию.
     ```python
     rdd2 = rdd.filter(lambda x: x % 2 == 0)
     ```

3. **Действия**: Это операции, которые запускают выполнение вычислений и возвращают результат. Примеры:
   - `collect`: собирает все элементы RDD в виде массива.
     ```python
     result = rdd.collect()
     ```
   - `count`: возвращает количество элементов в RDD.
     ```python
     count = rdd.count()
     ```

4. **Ленивое вычисление (Lazy Evaluation)**: Spark не выполняет трансформации немедленно, а откладывает выполнение до тех пор, пока не будет вызвана операция действия. Это позволяет Spark оптимизировать выполнение, снижая количество проходов по данным.

5. **Кэширование и персистенция**: RDD можно кэшировать в памяти для ускорения повторного использования данных.
   ```python
   rdd.cache()
   ```

RDD — это мощный инструмент для работы с большими данными, который позволяет распределённо и эффективно обрабатывать данные, сохраняя отказоустойчивость и высокую производительность.