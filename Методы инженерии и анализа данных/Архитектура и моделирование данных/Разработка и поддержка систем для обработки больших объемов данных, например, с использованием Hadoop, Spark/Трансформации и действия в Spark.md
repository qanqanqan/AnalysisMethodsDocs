### Трансформации и действия в Spark

В Apache Spark вычисления делятся на два основных типа операций: **трансформации (transformations)** и **действия (actions)**. Эти операции выполняются над RDD (Resilient Distributed Dataset) и DataFrame и являются основой для работы с данными в Spark. Чтобы эффективно использовать Spark, необходимо понимать различия между этими двумя типами операций.

#### 1. **Трансформации (Transformations)**

**Трансформации** — это операции, которые создают новый RDD или DataFrame на основе существующего. Они **ленивые** (lazy), то есть не выполняются немедленно, а откладываются до тех пор, пока не будет вызвана операция действия (action). Каждая трансформация порождает новый RDD, сохраняя информацию о зависимости от предыдущих RDD (это называется lineage — родословная данных). Трансформации в Spark всегда создают новые данные, не изменяя оригинальные.

##### Основные трансформации в Spark:

1. **map(func)**  
   Применяет функцию к каждому элементу RDD и возвращает новый RDD.  
   Пример:
   ```python
   rdd.map(lambda x: x * 2)
   ```

2. **filter(func)**  
   Возвращает новый RDD, состоящий только из элементов, которые удовлетворяют условию в функции.  
   Пример:
   ```python
   rdd.filter(lambda x: x > 10)
   ```

3. **flatMap(func)**  
   Похож на `map`, но каждая входная строка может быть преобразована в 0 или более выходных элементов.  
   Пример:
   ```python
   rdd.flatMap(lambda x: x.split(" "))
   ```

4. **groupByKey()**  
   Группирует данные по ключам (используется с RDD, состоящими из пар ключ-значение).  
   Пример:
   ```python
   rdd.groupByKey()
   ```

5. **reduceByKey(func)**  
   Похоже на `reduce()`, но применяется к каждой паре (ключ, значения) для агрегации значений с одинаковыми ключами.  
   Пример:
   ```python
   rdd.reduceByKey(lambda a, b: a + b)
   ```

6. **join(otherRdd)**  
   Выполняет внутреннее объединение двух RDD по ключам.  
   Пример:
   ```python
   rdd1.join(rdd2)
   ```

7. **union(otherRdd)**  
   Объединяет два RDD, возвращая новый RDD, содержащий все элементы обоих.  
   Пример:
   ```python
   rdd1.union(rdd2)
   ```

8. **distinct()**  
   Возвращает новый RDD, содержащий только уникальные элементы исходного RDD.  
   Пример:
   ```python
   rdd.distinct()
   ```

#### 2. **Действия (Actions)**

**Действия** — это операции, которые заставляют Spark выполнять вычисления и возвращают результат на драйвер или записывают его в файловую систему. В отличие от трансформаций, действия запускают реальное выполнение всего плана вычислений, накопленного до этого с помощью ленивых трансформаций. При вызове действия Spark выполняет все отложенные операции и вычисляет конечный результат.

##### Основные действия в Spark:

1. **collect()**  
   Возвращает все элементы RDD или DataFrame в виде массива на драйвере. Это действие следует использовать осторожно при работе с большими данными, так как вся коллекция загружается в память.  
   Пример:
   ```python
   rdd.collect()
   ```

2. **count()**  
   Возвращает количество элементов в RDD или DataFrame.  
   Пример:
   ```python
   rdd.count()
   ```

3. **reduce(func)**  
   Применяет функцию для агрегирования всех элементов RDD, возвращая одно значение.  
   Пример:
   ```python
   rdd.reduce(lambda a, b: a + b)
   ```

4. **take(n)**  
   Возвращает первые `n` элементов RDD. Это полезно для тестирования и отладки.  
   Пример:
   ```python
   rdd.take(5)
   ```

5. **first()**  
   Возвращает первый элемент RDD.  
   Пример:
   ```python
   rdd.first()
   ```

6. **saveAsTextFile(path)**  
   Сохраняет данные RDD в файл в виде текста.  
   Пример:
   ```python
   rdd.saveAsTextFile("/path/to/output")
   ```

7. **countByKey()**  
   Возвращает количество записей для каждого ключа в RDD, содержащем пары (ключ, значение).  
   Пример:
   ```python
   rdd.countByKey()
   ```

8. **foreach(func)**  
   Применяет функцию к каждому элементу RDD. В отличие от других действий, `foreach()` не возвращает результат на драйвер.  
   Пример:
   ```python
   rdd.foreach(lambda x: print(x))
   ```

#### 3. **Пример выполнения трансформаций и действий**

Для лучшего понимания рассмотрим простой пример:

```python
# Создаём RDD из списка чисел
rdd = sc.parallelize([1, 2, 3, 4, 5])

# Трансформация: умножение каждого элемента на 2
transformed_rdd = rdd.map(lambda x: x * 2)

# Действие: получение первых 3 элементов преобразованного RDD
result = transformed_rdd.take(3)

print(result)  # Вывод: [2, 4, 6]
```

В этом примере трансформация `map()` применяет функцию умножения ко всем элементам, но реальное вычисление не происходит, пока не вызвано действие `take()`, которое запрашивает первые 3 элемента.

#### 4. **Ленивое вычисление (Lazy Evaluation)**

Одной из ключевых особенностей Spark является **ленивое вычисление**. Это значит, что Spark не выполняет трансформации сразу после их вызова. Вместо этого Spark строит **граф вычислений** (DAG — Directed Acyclic Graph), в котором сохраняется последовательность всех операций. Реальное выполнение начинается только тогда, когда вызывается действие (action), которое требует результат.

##### Пример:
```python
# Эти строки лишь строят план вычислений, но не выполняются
rdd = sc.textFile("hdfs://path/to/file")
words = rdd.flatMap(lambda line: line.split(" "))
filtered_words = words.filter(lambda word: word.startswith("a"))

# Действие запускает выполнение всей цепочки
filtered_words.collect()
```

В этом примере данные из файла не будут загружены и обработаны до тех пор, пока не будет вызвана операция `collect()`.

#### 5. **Преимущества трансформаций и действий**

- **Трансформации** позволяют пользователям создавать сложные цепочки операций, но выполнение их откладывается, что экономит ресурсы до тех пор, пока не потребуется результат.
- **Действия** предоставляют способ получать результат и передавать его на драйвер или сохранять в файловую систему. Именно действия инициируют выполнение всех накопленных трансформаций.

### Заключение

**Трансформации** и **действия** — это основополагающие понятия в Spark. Трансформации служат для описания того, как данные должны быть обработаны, а действия — для выполнения вычислений и получения результатов. Понимание этих двух типов операций и их различий помогает эффективно использовать Spark для обработки больших данных.