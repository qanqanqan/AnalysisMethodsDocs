Пропущенные данные могут возникать по разным причинам, например, из-за ошибок в сборе данных или отсутствия информации. Важно правильно обработать такие данные, чтобы не исказить результаты анализа.

### Типы пропущенных данных:
1. **MCAR (Missing Completely at Random)** — данные пропущены полностью случайным образом, и вероятность отсутствия значения не зависит от других данных. В таких случаях можно просто удалить или заменить пропуски.
   
2. **MAR (Missing at Random)** — вероятность пропуска данных зависит от других наблюдаемых переменных. Например, если возраст человека может влиять на вероятность того, что он не указал свой доход.

3. **MNAR (Missing Not at Random)** — данные пропущены не случайно, и пропуски зависят от самой переменной. Например, люди с низким доходом могут избегать его указания.

### Методы заполнения пропущенных данных:

### 1. **Удаление пропущенных данных**

- **Удаление строк с пропущенными значениями**:
   Если в строке есть хотя бы одно пропущенное значение, строку можно удалить.
   
   Пример:
   ```python
   from pyspark.sql import SparkSession
   
   # Инициализация SparkSession
   spark = SparkSession.builder.appName("DataCleaning").getOrCreate()
   
   # Пример данных
   data = [("Иван", 28, None), ("Ольга", None, "Россия"), ("Анна", 32, "США")]
   columns = ["Имя", "Возраст", "Страна"]
   
   df = spark.createDataFrame(data, columns)
   
   # Удаление строк с любыми пропущенными значениями
   df_clean = df.na.drop()
   df_clean.show()
   ```

   Результат:
   ```
   +----+-------+------+
   |Имя |Возраст|Страна|
   +----+-------+------+
   |Анна|     32|  США |
   +----+-------+------+
   ```

- **Удаление строк, где все значения пропущены**:
   Можно удалять только те строки, где пропущены все значения.
   
   Пример:
   ```python
   # Удаление строк, где все значения пропущены
   df_clean = df.na.drop(how="all")
   df_clean.show()
   ```

### 2. **Заполнение пропущенных значений**

- **Заполнение постоянным значением**:
   Например, можно заменить все пропущенные значения в столбце "Страна" на "Неизвестно".
   
   Пример:
   ```python
   # Заполнение пропущенных значений в столбце "Страна" значением "Неизвестно"
   df_filled = df.na.fill("Неизвестно", subset=["Страна"])
   df_filled.show()
   ```

   Результат:
   ```
   +----+-------+----------+
   |Имя |Возраст|Страна    |
   +----+-------+----------+
   |Иван|     28|Неизвестно|
   |Ольга|   null|Неизвестно|
   |Анна|     32|США       |
   +----+-------+----------+
   ```

- **Заполнение средним, медианой или модой**:
   Для числовых данных можно заполнить пропуски средним значением. В PySpark нет прямой функции для вычисления среднего и заполнения, но можно сделать это через несколько шагов.

   Пример:
   ```python
   from pyspark.sql.functions import mean

   # Вычисление среднего по столбцу "Возраст"
   mean_age = df.select(mean("Возраст")).collect()[0][0]
   
   # Заполнение пропущенных значений среднего по "Возраст"
   df_filled = df.na.fill(mean_age, subset=["Возраст"])
   df_filled.show()
   ```

   Результат:
   ```
   +----+-------+------+
   |Имя |Возраст|Страна|
   +----+-------+------+
   |Иван|   28.0|  null|
   |Ольга|   30.0|Россия|
   |Анна|   32.0|   США|
   +----+-------+------+
   ```

### 3. **Заполнение с использованием предыдущих или последующих значений**

PySpark не поддерживает методы прямого заполнения предыдущими или последующими значениями (как в pandas), но это можно сделать через создание оконных функций.

Пример:
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import last

# Оконная функция для заполнения предыдущими значениями
window_spec = Window.partitionBy("Имя").orderBy("Возраст").rowsBetween(-1, 0)

# Заполнение предыдущими значениями
df_filled = df.withColumn("Возраст", last("Возраст", True).over(window_spec))
df_filled.show()
```

### 4. **Интерполяция (линейная интерполяция)**

В PySpark нет встроенной функции для линейной интерполяции, как в pandas. Однако можно реализовать ее через оконные функции и собственные вычисления, используя UDF (user-defined functions).

### 5. **Заполнение на основе моделей (регрессия)**

PySpark поддерживает использование моделей машинного обучения для предсказания и заполнения пропущенных значений.

Пример: использование линейной регрессии для заполнения пропущенных значений в столбце "Возраст":
```python
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

# Исключаем строки с пропущенными значениями для обучения
train_df = df.filter(df["Возраст"].isNotNull())

# Подготовка данных для модели
assembler = VectorAssembler(inputCols=["Страна"], outputCol="features")
train_df = assembler.transform(train_df)

# Обучение модели линейной регрессии
lr = LinearRegression(featuresCol="features", labelCol="Возраст")
lr_model = lr.fit(train_df)

# Применение модели для заполнения пропущенных значений
test_df = df.filter(df["Возраст"].isNull())
test_df = assembler.transform(test_df)
predictions = lr_model.transform(test_df)

predictions.select("Имя", "prediction").show()
```

Этот пример демонстрирует использование линейной регрессии для предсказания отсутствующих значений.